name: S1 Scraper

on:
  workflow_call:
    inputs:
      SHARD_STEP:
        required: false
        type: number
        default: 20

jobs:
  scrape:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        shard: [0,1,2,3,4]
    env:
      SHARD_STEP: ${{ inputs.SHARD_STEP }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      - run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable
      - run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 gspread webdriver-manager pandas requests openpyxl
      - run: echo '${{ secrets.GSPREAD_CREDENTIALS }}' > credentials.json
      - run: echo '${{ secrets.TRADINGVIEW_COOKIES }}' > cookies.json
      - run: |
          FILE="checkpoint_s1_${{ matrix.shard }}.txt"
          if [ ! -f "$FILE" ]; then echo "1" > "$FILE"; fi
      - env:
          SHARD_INDEX: ${{ matrix.shard }}
          CHECKPOINT_FILE: checkpoint_s1_${{ matrix.shard }}.txt
        run: python run_scraper.py --scraper S1 --shard ${{ matrix.shard }} --checkpoint $CHECKPOINT_FILE
